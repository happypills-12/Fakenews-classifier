{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment 1\n",
    "<h2 align='center'> Detecting Fake news </h2>\n",
    "\n",
    "**Names :** Vaishali Raja, Ludwig Orsini-Rosenberg, Nicolas Linsenmaier\n",
    "\n",
    "**Best Score : ** 94.7% (2nd : 94.3%)\n",
    "\n",
    "**Classifier Used : ** Passive-Aggressive Classifier (2nd : SGDC with GridSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<align = 'center'> \n",
    "The objective of this assignment was to create a classifier that is able to detect and differentiate fake news from real news based on NLP techniques. The main pre-processing steps used in this assignment involves basic ideas such as removing stop words, converting to lowercase, stemming , working with punctuation and using n-grams. Following this, features would be extracted and transformed primarily through scikit-learn's TF-IDF vectorizer which is a combination of Count Vectorizer (Bag of Words) + a TF-IDF Transformer. The extracted features would then be applied to 3 classifiers : Multinomial Naive Bayes, Stochastic Gradient Descent Classifier and Passive-Aggresive Classifier using cross validation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "    1. Import necessary packages\n",
    "    2. Import Train and Test Data\n",
    "    3. Clean Train Data\n",
    "        3.1 Split dataset \n",
    "        3.2 Process X2 column\n",
    "        3.2 Process X1 column\n",
    "        3.3 Merge and create final train dataset\n",
    "    4. Base analysis\n",
    "        4.1 Levels\n",
    "        4.2 Frequency distribution and Vocab\n",
    "    5. Tokenizer and Vectorizer\n",
    "    6. Multinomial Naive Bayes\n",
    "    7. Stochastic Gradient Descent Classifier\n",
    "    8. Passive-Aggressive Classifier\n",
    "    9. Applying classifier on Test Data\n",
    "    10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./fake_or_real_news_training.csv\", encoding = \"UTF-8\" )\n",
    "test = pd.read_csv(\"./fake_or_real_news_test.csv\", encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: \n",
      "\n",
      "Number of columns: 6\n",
      "Number of rows: 3999\n",
      "Train Data: \n",
      "\n",
      "Number of columns: 3\n",
      "Number of rows: 2321\n"
     ]
    }
   ],
   "source": [
    "print('Train Data: \\n')\n",
    "print(\"Number of columns: \"+ str(train.shape[1]))\n",
    "print(\"Number of rows: \"+ str(train.shape[0]))\n",
    "\n",
    "print('Train Data: \\n')\n",
    "print(\"Number of columns: \"+ str(test.shape[1]))\n",
    "print(\"Number of rows: \"+ str(test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing % of NA in Num variable: \n",
      "\n",
      "Table:     Missing % of NA in Numerical Var\n",
      "X2                         99.949987\n",
      "X1                         99.174794\n"
     ]
    }
   ],
   "source": [
    "#Percentage of missing values in Numerical Variables\n",
    "data_num_na = (train.isnull().sum() / len(train)) * 100\n",
    "data_num_na = data_num_na.drop(data_num_na[data_num_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data_n = pd.DataFrame({'Missing % of NA in Numerical Var' :data_num_na})\n",
    "print('\\n Missing % of NA in Num variable: \\n')\n",
    "print(\"Table: \" + str(missing_data_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Count of values in col X1: \n",
      "\n",
      "31\n",
      "\n",
      " Count of values in col X2: \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into 3 set; Clean - Data has been parsed correctly\n",
    "#rows_x1 - Data has been split and moved into x1\n",
    "#rows_x2 - Data has been split into both X1 and X2 columns\n",
    "\n",
    "clean = train[train['X1'].isnull()]\n",
    "rows_x1 = train[train['X1'].notnull()& train['X2'].isnull()]\n",
    "rows_x2 = train[train['X2'].notnull()]\n",
    "\n",
    "print('\\n Count of values in col X1: \\n')\n",
    "print(str(rows_x1.shape[0]) + \"\\n\" + \"\\n Count of values in col X2: \\n\" + str (rows_x2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 31 rows which have been extended into Col X1 and 2 rows which have been extended twice into Col X1 and X2. We will now take a look at these data points and work to make it fit within the first 4 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Processing X2 Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>9</td>\n",
       "      <td>Planned Parenthood’s lobbying effort</td>\n",
       "      <td>pay raises for federal workers</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>6268</td>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                              title  \\\n",
       "2184     9               Planned Parenthood’s lobbying effort   \n",
       "3537  6268  Chart Of The Day: Since 2009—–Recovery For The 5%   \n",
       "\n",
       "                                 text  \\\n",
       "2184   pay raises for federal workers   \n",
       "3537           Stagnation for the 95%   \n",
       "\n",
       "                                                 label  \\\n",
       "2184                          and the future Fed rates   \n",
       "3537  Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "\n",
       "                                                     X1    X2  \n",
       "2184  PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  \n",
       "3537                           Stagnation for the 95%    FAKE  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks the values in the title have been shifted into the text(&label) columns causing values to appear in the X1 and X2 columns. We are going to investigate the columns that have been shifted twice and proceed further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Planned Parenthood’s lobbying effort\n",
      "Text :  pay raises for federal workers\n",
      "Label :  and the future Fed rates\n"
     ]
    }
   ],
   "source": [
    "print(\"Title\" + \" : \" + rows_x2.iloc[0,1])\n",
    "print(\"Text\" + \" : \" + rows_x2.iloc[0,2])\n",
    "print(\"Label\" + \" : \" + rows_x2.iloc[0,3])\n",
    "#print(\"X1\" + \" : \" + rows_x2.iloc[0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Chart Of The Day: Since 2009—–Recovery For The 5%\n",
      "Text :  Stagnation for the 95%\n",
      "Label : Chart Of The Day: Since 2009 Recovery For The 5%\n"
     ]
    }
   ],
   "source": [
    "print(\"Title\" + \" : \" + rows_x2.iloc[1,1])\n",
    "print(\"Text\" + \" : \" + rows_x2.iloc[1,2])\n",
    "print(\"Label\" + \" : \" + rows_x2.iloc[1,3])\n",
    "#print(\"X1\" + \" : \" + rows_x2.iloc[1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the text, we are going to drop input no:3537 as it does not have any real value for the text section. As for ID no:2184, we are going to move the values 2 colums back as the Title has been shifted over 3 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop ID: 3537\n",
    "rows_x2 = rows_x2.drop(3537)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>9</td>\n",
       "      <td>Planned Parenthood’s lobbying effort pay raise...</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "2184   9  Planned Parenthood’s lobbying effort pay raise...   \n",
       "\n",
       "                                                   text label   X1   X2  \n",
       "2184  PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shift the other row 2 columns back\n",
    "rows_x2.title.iloc[0] = rows_x2.title.iloc[0] + rows_x2.text.iloc[0] + rows_x2.label.iloc[0]\n",
    "rows_x2.iloc[0,2:] = rows_x2.iloc[0,2:].shift(-2)\n",
    "rows_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Processing X1 Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>599</td>\n",
       "      <td>Election Day: No Legal Pot In Ohio</td>\n",
       "      <td>Democrats Lose In The South</td>\n",
       "      <td>Election Day: No Legal Pot In Ohio; Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>10194</td>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>Leonardo DiCaprio to the rescue?</td>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>356</td>\n",
       "      <td>Black Hawk crashes off Florida</td>\n",
       "      <td>human remains found</td>\n",
       "      <td>(CNN) Thick fog forced authorities to suspend ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>2786</td>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital</td>\n",
       "      <td>U.S. investigating</td>\n",
       "      <td>(CNN) Aerial bombardments blew apart a Doctors...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>3622</td>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>US issues travel warning</td>\n",
       "      <td>A member of Al Qaeda's branch in Yemen said Fr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>7375</td>\n",
       "      <td>Shallow 5.4 magnitude earthquake rattles centr...</td>\n",
       "      <td>shakes buildings in Rome</td>\n",
       "      <td>00 UTC © USGS Map of the earthquake's epicent...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>9097</td>\n",
       "      <td>ICE Agent Commits Suicide in NYC</td>\n",
       "      <td>Leaves Note Revealing Gov’t Plans to Round-up...</td>\n",
       "      <td>Email Print After writing a lengthy suicide no...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>9203</td>\n",
       "      <td>Political Correctness for Yuengling Brewery</td>\n",
       "      <td>What About Our Opioid Epidemic?</td>\n",
       "      <td>We Are Change \\n\\nIn today’s political climate...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>1602</td>\n",
       "      <td>Poll gives Biden edge over Clinton against GOP...</td>\n",
       "      <td>VP meets with Trumka</td>\n",
       "      <td>A new national poll shows Vice President Biden...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>4562</td>\n",
       "      <td>Russia begins airstrikes in Syria</td>\n",
       "      <td>U.S. warns of new concerns in conflict</td>\n",
       "      <td>Russian warplanes began airstrikes in Syria on...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>4748</td>\n",
       "      <td>Trump &amp;amp</td>\n",
       "      <td>Clinton Were Very Convincing...on How Lousy t...</td>\n",
       "      <td>Let's pretend for a moment that the biggest he...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>3508</td>\n",
       "      <td>Belgian police mount raids</td>\n",
       "      <td>prosecutors acknowledge missed opportunities</td>\n",
       "      <td>Belgian authorities missed a chance to press a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>7559</td>\n",
       "      <td>STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...</td>\n",
       "      <td>GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS</td>\n",
       "      <td>Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>3634</td>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues</td>\n",
       "      <td>Brothers Were On No-Fly List</td>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues;...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>8470</td>\n",
       "      <td>The Amish In America Commit Their Vote To Dona...</td>\n",
       "      <td>Mathematically Guaranteeing Him A Presidentia...</td>\n",
       "      <td>18 SHARE The Amish in America have committed t...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>6404</td>\n",
       "      <td>#BREAKING: SECOND Assassination Attempt On Tru...</td>\n",
       "      <td>Suspect Detained (LIVE BLOG)</td>\n",
       "      <td>We Are Change \\nDonald Trump on Saturday was q...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>10499</td>\n",
       "      <td>30th Infantry Division: “Work Horse of the Wes...</td>\n",
       "      <td>The Big Picture TV-211</td>\n",
       "      <td>Published on Oct 27, 2016 by Jeff Quitney The ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>10492</td>\n",
       "      <td>TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...</td>\n",
       "      <td>“THE END OF LIFE AS WE KNOW IT”</td>\n",
       "      <td>Paul Joseph Watson Senior British army officer...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>10138</td>\n",
       "      <td>Inside The Mind Of An FBI Informant</td>\n",
       "      <td>Terri Linnell Admits Role As Gov’t Snitch</td>\n",
       "      <td>Inside The Mind Of An FBI Informant; Terri Lin...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>4953</td>\n",
       "      <td>Gary Johnson Avoids Typical Third-Party Fade</td>\n",
       "      <td>Best Polling Since Perot in ‘92</td>\n",
       "      <td>A couple of weeks ago in this space I pushed b...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>496</td>\n",
       "      <td>Nearly 300K New Jobs In February</td>\n",
       "      <td>Unemployment Dips To 5.5 Percent</td>\n",
       "      <td>Nearly 300K New Jobs In February; Unemployment...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>5741</td>\n",
       "      <td>Why Trump Won</td>\n",
       "      <td>Why Clinton Lost</td>\n",
       "      <td>WashingtonsBlog \\nBy Robert Parry, the inves...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>4131</td>\n",
       "      <td>Jesse Matthew charged in Hannah Graham's murder</td>\n",
       "      <td>DA will not pursue death penalty</td>\n",
       "      <td>Jesse Matthew Jr., a former hospital worker, w...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>8748</td>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRiot</td>\n",
       "      <td>Media Ignores (Video)</td>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRio...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>6717</td>\n",
       "      <td>Jim Rogers: It’s Time To Prepare</td>\n",
       "      <td>Economic And Financial Collapse Imminent (VIDEO)</td>\n",
       "      <td>By: The Voice of Reason | Regardless of how mu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>2943</td>\n",
       "      <td>Islamic State admits defeat in Kobani</td>\n",
       "      <td>blames airstrikes</td>\n",
       "      <td>Islamic State militants have acknowledged for ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>5248</td>\n",
       "      <td>Clinton Cries Racism Tagging Trump with KKK</td>\n",
       "      <td>Trump Says 'She Lies'</td>\n",
       "      <td>With only about 70 days left until the electio...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>3624</td>\n",
       "      <td>Suspects In Paris Magazine Attack Killed</td>\n",
       "      <td>Market Gunman And 4 Hostages Also Dead</td>\n",
       "      <td>Suspects In Paris Magazine Attack Killed; Mark...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>2738</td>\n",
       "      <td>Ted Cruz launches bid</td>\n",
       "      <td>Some pundits paint him as scary extremist</td>\n",
       "      <td>Before he got to repealing ObamaCare, before h...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>4025</td>\n",
       "      <td>State Dept. IDs 2 Americans killed in Nepal quake</td>\n",
       "      <td>2 others reportedly dead</td>\n",
       "      <td>The State Department identified two Americans ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>9954</td>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>bursting of firecrackers suspected</td>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                              title  \\\n",
       "192     599                 Election Day: No Legal Pot In Ohio   \n",
       "308   10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "382     356                     Black Hawk crashes off Florida   \n",
       "660    2786     Afghanistan: 19 die in air attacks on hospital   \n",
       "889    3622  Al Qaeda rep says group directed Paris magazin...   \n",
       "911    7375  Shallow 5.4 magnitude earthquake rattles centr...   \n",
       "1010   9097                   ICE Agent Commits Suicide in NYC   \n",
       "1043   9203        Political Correctness for Yuengling Brewery   \n",
       "1218   1602  Poll gives Biden edge over Clinton against GOP...   \n",
       "1438   4562                  Russia begins airstrikes in Syria   \n",
       "1493   4748                                         Trump &amp   \n",
       "1591   3508                         Belgian police mount raids   \n",
       "1630   7559  STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...   \n",
       "1716   3634      The Latest On Paris Attack: Manhunt Continues   \n",
       "1900   8470  The Amish In America Commit Their Vote To Dona...   \n",
       "1968   6404  #BREAKING: SECOND Assassination Attempt On Tru...   \n",
       "2176  10499  30th Infantry Division: “Work Horse of the Wes...   \n",
       "2493  10492  TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...   \n",
       "2549  10138                Inside The Mind Of An FBI Informant   \n",
       "2880   4953       Gary Johnson Avoids Typical Third-Party Fade   \n",
       "2920    496                   Nearly 300K New Jobs In February   \n",
       "3010   5741                                      Why Trump Won   \n",
       "3069   4131    Jesse Matthew charged in Hannah Graham's murder   \n",
       "3110   8748      WATCH: Mass Shooting Occurs During #TrumpRiot   \n",
       "3130   6717                   Jim Rogers: It’s Time To Prepare   \n",
       "3210   2943              Islamic State admits defeat in Kobani   \n",
       "3372   5248        Clinton Cries Racism Tagging Trump with KKK   \n",
       "3478   3624           Suspects In Paris Magazine Attack Killed   \n",
       "3578   2738                              Ted Cruz launches bid   \n",
       "3649   4025  State Dept. IDs 2 Americans killed in Nepal quake   \n",
       "3706   9954  Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                   text  \\\n",
       "192                         Democrats Lose In The South   \n",
       "308                    Leonardo DiCaprio to the rescue?   \n",
       "382                                 human remains found   \n",
       "660                                  U.S. investigating   \n",
       "889                            US issues travel warning   \n",
       "911                            shakes buildings in Rome   \n",
       "1010   Leaves Note Revealing Gov’t Plans to Round-up...   \n",
       "1043                    What About Our Opioid Epidemic?   \n",
       "1218                               VP meets with Trumka   \n",
       "1438             U.S. warns of new concerns in conflict   \n",
       "1493   Clinton Were Very Convincing...on How Lousy t...   \n",
       "1591       prosecutors acknowledge missed opportunities   \n",
       "1630       GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS   \n",
       "1716                       Brothers Were On No-Fly List   \n",
       "1900   Mathematically Guaranteeing Him A Presidentia...   \n",
       "1968                       Suspect Detained (LIVE BLOG)   \n",
       "2176                             The Big Picture TV-211   \n",
       "2493                    “THE END OF LIFE AS WE KNOW IT”   \n",
       "2549          Terri Linnell Admits Role As Gov’t Snitch   \n",
       "2880                    Best Polling Since Perot in ‘92   \n",
       "2920                   Unemployment Dips To 5.5 Percent   \n",
       "3010                                   Why Clinton Lost   \n",
       "3069                   DA will not pursue death penalty   \n",
       "3110                              Media Ignores (Video)   \n",
       "3130   Economic And Financial Collapse Imminent (VIDEO)   \n",
       "3210                                  blames airstrikes   \n",
       "3372                              Trump Says 'She Lies'   \n",
       "3478             Market Gunman And 4 Hostages Also Dead   \n",
       "3578          Some pundits paint him as scary extremist   \n",
       "3649                           2 others reportedly dead   \n",
       "3706                 bursting of firecrackers suspected   \n",
       "\n",
       "                                                  label    X1   X2  \n",
       "192   Election Day: No Legal Pot In Ohio; Democrats ...  REAL  NaN  \n",
       "308   Who rode it best? Jesse Jackson mounts up to f...  FAKE  NaN  \n",
       "382   (CNN) Thick fog forced authorities to suspend ...  REAL  NaN  \n",
       "660   (CNN) Aerial bombardments blew apart a Doctors...  REAL  NaN  \n",
       "889   A member of Al Qaeda's branch in Yemen said Fr...  REAL  NaN  \n",
       "911    00 UTC © USGS Map of the earthquake's epicent...  FAKE  NaN  \n",
       "1010  Email Print After writing a lengthy suicide no...  FAKE  NaN  \n",
       "1043  We Are Change \\n\\nIn today’s political climate...  FAKE  NaN  \n",
       "1218  A new national poll shows Vice President Biden...  REAL  NaN  \n",
       "1438  Russian warplanes began airstrikes in Syria on...  REAL  NaN  \n",
       "1493  Let's pretend for a moment that the biggest he...  REAL  NaN  \n",
       "1591  Belgian authorities missed a chance to press a...  REAL  NaN  \n",
       "1630  Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...  FAKE  NaN  \n",
       "1716  The Latest On Paris Attack: Manhunt Continues;...  REAL  NaN  \n",
       "1900  18 SHARE The Amish in America have committed t...  FAKE  NaN  \n",
       "1968  We Are Change \\nDonald Trump on Saturday was q...  FAKE  NaN  \n",
       "2176  Published on Oct 27, 2016 by Jeff Quitney The ...  FAKE  NaN  \n",
       "2493  Paul Joseph Watson Senior British army officer...  FAKE  NaN  \n",
       "2549  Inside The Mind Of An FBI Informant; Terri Lin...  FAKE  NaN  \n",
       "2880  A couple of weeks ago in this space I pushed b...  REAL  NaN  \n",
       "2920  Nearly 300K New Jobs In February; Unemployment...  REAL  NaN  \n",
       "3010    WashingtonsBlog \\nBy Robert Parry, the inves...  FAKE  NaN  \n",
       "3069  Jesse Matthew Jr., a former hospital worker, w...  REAL  NaN  \n",
       "3110    WATCH: Mass Shooting Occurs During #TrumpRio...  FAKE  NaN  \n",
       "3130  By: The Voice of Reason | Regardless of how mu...  FAKE  NaN  \n",
       "3210  Islamic State militants have acknowledged for ...  REAL  NaN  \n",
       "3372  With only about 70 days left until the electio...  REAL  NaN  \n",
       "3478  Suspects In Paris Magazine Attack Killed; Mark...  REAL  NaN  \n",
       "3578  Before he got to repealing ObamaCare, before h...  REAL  NaN  \n",
       "3649  The State Department identified two Americans ...  REAL  NaN  \n",
       "3706  Incredible smoke haze seen outside NDTV office...  FAKE  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS FAITH\n",
      "Text :  GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS\n",
      "X1 : FAKE\n"
     ]
    }
   ],
   "source": [
    "print(\"Title\" + \" : \" + rows_x1.iloc[12,1])\n",
    "print(\"Text\" + \" : \" + rows_x1.iloc[12,2])\n",
    "#print(\"Label\" + \" : \" + rows_x1.iloc[12,3])\n",
    "print(\"X1\" + \" : \" + rows_x1.iloc[12,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a sample of texts, it seems like the data from title has been split into both the title and text columns. As such we will be moving these data points one columns back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS FAITH: GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS\n",
      "Text :  GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "#Appending title and text in title column\n",
    "rows_x1.title = rows_x1.title + ':' + rows_x1.text\n",
    "#checking sample\n",
    "print(\"Title\" + \" : \" + rows_x1.iloc[12,1])\n",
    "print(\"Text\" + \" : \" + rows_x1.iloc[12,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting one column back\n",
    "cols = rows_x1.columns[:-1]\n",
    "rows_x1 = rows_x1.drop('text', 1)\n",
    "rows_x1.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS FAITH: GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS\n",
      "Label : FAKE\n"
     ]
    }
   ],
   "source": [
    "print(\"Title\" + \" : \" + rows_x1.iloc[12,1])\n",
    "#print(\"Text\" + \" : \" + rows_x1.iloc[12,2])\n",
    "print(\"Label\" + \" : \" + rows_x1.iloc[12,3])\n",
    "#print(\"X1\" + \" : \" + rows_x1.iloc[12,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Merge and create final train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop X1 and X2 columns as they are not required further\n",
    "rows_x1 = rows_x1.drop('X1', 1)\n",
    "rows_x2 = rows_x2.drop(['X1','X2'],1)\n",
    "clean = clean.drop(['X1','X2'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge cleaned datasets together\n",
    "train_l = pd.concat([clean,rows_x1,rows_x2])\n",
    "\n",
    "#Create a new level with both title and text\n",
    "train_l['both'] = train_l.title + train_l.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_l.label\n",
    "train = train_l.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Base analysis \n",
    "\n",
    "### 4.1 Levels\n",
    "The original input data has 2 levels: 'Title' and 'Text'. With the final train dataset, a new level,'both' has been created which combines both the Title and the Text of the news article. \n",
    "\n",
    "In this section, we will be performing a base analysis on 2 levels : 'Text' and'both' using a simple TF-TDF vectorizer on Multinomial Naive Bayes Classifier to decide on which level to use for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.815      0.79375    0.8075     0.80225282 0.80225282]\n",
      "Mean Acc = 0.80415112640801\n"
     ]
    }
   ],
   "source": [
    "text_pip = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                     ('Multi_NB', MultinomialNB())])\n",
    "\n",
    "accuracy = cross_val_score(text_pip, train.text, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.82       0.7975     0.81125    0.81476846 0.80350438]\n",
      "Mean Acc = 0.8094045682102629\n"
     ]
    }
   ],
   "source": [
    "both_pip = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                     ('Multi_NB', MultinomialNB())])\n",
    "\n",
    "accuracy = cross_val_score(both_pip, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both the text and title is marginally better and it makes sense as it would mean that features from both levels will be included in the analysis and would enhance classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Frequency distribution and Vocab\n",
    "\n",
    "This section is to have a basic understanding of the differences in frequency distribution of words and vocab between the Fake and Real news articles. The process was done step-by-step and slight differences could only be observed after stopwords and single-character tokens were removed. \n",
    "\n",
    "The vocab set between Fake and Real datasets do not show much difference, but some sort of sentiment analysis can be applied to gather more insights on the type of vocab used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fake = train_l[train_l['label']=='FAKE']\n",
    "Real = train_l[train_l['label']=='REAL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_f = str(Fake['both'])\n",
    "str_r = str(Real['both'])\n",
    "type(str_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|   Word   | Word Count |\n",
      "+----------+------------+\n",
      "|   ...    |     61     |\n",
      "|  Trump   |     7      |\n",
      "| Hillary  |     6      |\n",
      "|   The    |     4      |\n",
      "|    To    |     4      |\n",
      "|   FBI    |     4      |\n",
      "| America  |     3      |\n",
      "|   Why    |     3      |\n",
      "|  Woman   |     3      |\n",
      "|    In    |     3      |\n",
      "|   This   |     3      |\n",
      "|    OF    |     3      |\n",
      "|   News   |     2      |\n",
      "|   War    |     2      |\n",
      "| Clinton  |     2      |\n",
      "|  After   |     2      |\n",
      "|    24    |     2      |\n",
      "|  Syrian  |     2      |\n",
      "| November |     2      |\n",
      "|   2016   |     2      |\n",
      "+----------+------------+\n",
      "Vocabulary in Fake News : 393\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(str_f)\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove stopwords\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "words = [word for word in words if word not in default_stopwords]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "# Output top 10 words    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Word\", \"Word Count\"]\n",
    "\n",
    "for word, frequency in fdist.most_common(20) :\n",
    "    x.add_row([word,frequency])\n",
    "\n",
    "print(x)\n",
    "\n",
    "words_l = [w.lower() for w in words]\n",
    "vocab = len(set(words))\n",
    "\n",
    "print(\"Vocabulary in Fake News : \" + str(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|   Word  | Word Count |\n",
      "+---------+------------+\n",
      "|   ...   |     62     |\n",
      "|  Trump  |     13     |\n",
      "| Clinton |     9      |\n",
      "|   GOP   |     6      |\n",
      "|   The   |     5      |\n",
      "|  Donald |     5      |\n",
      "|  Paris  |     4      |\n",
      "|  Obama  |     4      |\n",
      "|    's   |     4      |\n",
      "|   New   |     3      |\n",
      "|  makes  |     3      |\n",
      "|   Cruz  |     3      |\n",
      "|    In   |     3      |\n",
      "|   Iran  |     2      |\n",
      "|   With  |     2      |\n",
      "|  women  |     2      |\n",
      "|    19   |     2      |\n",
      "| Hillary |     2      |\n",
      "|  Senate |     2      |\n",
      "|   His   |     2      |\n",
      "+---------+------------+\n",
      "Vocabulary in Real News : 368\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(str_r)\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove stopwords\n",
    "words = [word for word in words if word not in default_stopwords]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "# Output top 10 words    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Word\", \"Word Count\"]\n",
    "\n",
    "for word, frequency in fdist.most_common(20) :\n",
    "    x.add_row([word,frequency])\n",
    "\n",
    "print(x)\n",
    "\n",
    "words_l = [w.lower() for w in words]\n",
    "vocab = len(set(words))\n",
    "\n",
    "print(\"Vocabulary in Real News : \" + str(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Defining Tokenizer and Vectorizer\n",
    "\n",
    "Defining the tokenizer here which takes into account stemming and punctuation. The vectorizer used is the TF-IDF vectorizer which extracts features using the count vectorizer and transforms it using a TF-IDF transformer. \n",
    "There are a number of inputs factors that can be changed to extract features that would provide the highest accuracy. Some of these input factors that would be looked into are : \n",
    "\n",
    "    1) Stop Words\n",
    "    2) Tokenizing with Stemming\n",
    "    3) Converting words to lowercase\n",
    "    4) N-grams of order 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        #if item in string.punctuation: continue\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base vectorizer with stopwords\n",
    "tfidf_vectorizer_sw = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Base vectorizer with tokenizer\n",
    "tfidf_vectorizer_tk = TfidfVectorizer(tokenizer = tokenize)\n",
    "\n",
    "# Base vectorizer with n-gram\n",
    "tfidf_vectorizer_try = TfidfVectorizer(sublinear_tf=True, min_df=5, \n",
    "                                       norm='l2', ngram_range=(1,2),\n",
    "                                       max_df = 0.7)\n",
    "\n",
    "# Base vectorizer with n-gram, stopwords\n",
    "tfidf_vectorizer_swn = TfidfVectorizer(stop_words='english',sublinear_tf=True,\n",
    "                                       min_df=5, norm='l2', \n",
    "                                       ngram_range=(1,2),max_df = 0.7)\n",
    "\n",
    "# Base vectorizer with n-gram, stopwords and tokenizer\n",
    "tfidf_vectorizer_swntk = TfidfVectorizer(stop_words='english', sublinear_tf=True, \n",
    "                                         min_df=5, norm='l2', tokenizer = tokenize,\n",
    "                                         ngram_range=(1,2), max_df = 0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes is the most commonly used classifier for text classification. For the first part, we will apply the MNB classifier on the various vectorizers created above and identify the best performing one. With that, we would further apply gridsearch to identify new features that would increase the accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.8425     0.8225     0.84375    0.83604506 0.82728411]\n",
      "Mean Acc = 0.834415832290363\n"
     ]
    }
   ],
   "source": [
    "text_clf_NB_1 = Pipeline([('tfidf', tfidf_vectorizer_sw),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-NB', MultinomialNB())])\n",
    "\n",
    "accuracy = cross_val_score(text_clf_NB_1, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_clf_NB_2 = Pipeline([('tfidf', tfidf_vectorizer_tk),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-NB', MultinomialNB())])\n",
    "\n",
    "#accuracy = cross_val_score(text_clf_NB_2, train.both, label, scoring='accuracy', cv=5)\n",
    "#print(\"Accuracy = \" + str(accuracy))\n",
    "#print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.9        0.89       0.90625    0.89486859 0.8873592 ]\n",
      "Mean Acc = 0.8956955569461827\n"
     ]
    }
   ],
   "source": [
    "text_clf_NB_3 = Pipeline([('tfidf', tfidf_vectorizer_try),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-NB', MultinomialNB())])\n",
    "\n",
    "accuracy = cross_val_score(text_clf_NB_3, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.91       0.9        0.90875    0.9048811  0.88235294]\n",
      "Mean Acc = 0.9011968085106382\n"
     ]
    }
   ],
   "source": [
    "text_clf_NB_4 = Pipeline([('tfidf', tfidf_vectorizer_swn),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-NB', MultinomialNB())])\n",
    "\n",
    "accuracy = cross_val_score(text_clf_NB_4, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_clf_NB_5 = Pipeline([('tfidf', tfidf_vectorizer_swntk),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-NB', MultinomialNB())])\n",
    "\n",
    "#accuracy = cross_val_score(text_clf_NB_4, train.both, label, scoring='accuracy', cv=5)\n",
    "#print(\"Accuracy = \" + str(accuracy))\n",
    "#print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the tests above, the best base vetorizer to use is **tfidf_vectorizer_swn** which includes **Stopwords and n-grams of order 2.** This will be used for further classifiers and will be updated based on the results obtained. \n",
    "It is to be noted that stemming does not provide better results here. This could be because stemming or cutting off the roots of the words is probably not optimal for Fake news detection as it may use different verb conjugations or change up the tenses as opposed to the more structured and formal Real news. \n",
    "\n",
    "*For the final run, the pipelines including stemming were not run as they took a long time to run. This is the same case for the gridsearches, but the results of a previous run is included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for MNB\n",
    "#parameters_NB = {'tfidf__use_idf': (True, False),\n",
    "#                  'tfidf__lowercase': (True,False),\n",
    "#                  'clf-NB__alpha': (0.01, 0.05),\n",
    "#                 }\n",
    "\n",
    "#gs_clf_NB = GridSearchCV(text_clf_NB_4, cv=5, param_grid=parameters_NB, n_jobs=-1)\n",
    "#gs_clf_NB = gs_clf_NB.fit(train.both, label)\n",
    "\n",
    "#print (gs_clf_NB.best_score_)\n",
    "#print (gs_clf_NB.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Grid Search is as follows: \n",
    "   \n",
    "       0.936468234117\n",
    "    {'clf-NB_alpha': 0.01, 'tfidflowercase': False, 'tfidf_use_idf': True}\n",
    "\n",
    "Although converting all words to lowercase is the usual normalization practise, in this case, it is not to be. It makes logical sense as Fake news would try to elicit heightened emotions by using excessive capitalization which would be very useful for classification purposes. With this result, we will amend **tfidf_vectorizer_swn** to include lowercase = False. Use_idf will not be changed as True is the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_swn = TfidfVectorizer(stop_words='english',sublinear_tf=True,\n",
    "                                       min_df=5, norm='l2', \n",
    "                                       ngram_range=(1,2),max_df = 0.7,\n",
    "                                       lowercase = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machine\n",
    "\n",
    "The most commonly used linear model in text classification is Support Vector Machine. There are 2 common classifiers (linear_svc and SGDC), both of which were tries and it was found that SGDC provided much better results as compared linear SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf_sgdc = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                    alpha=1e-3, random_state=42,\n",
    "                    max_iter=5, tol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.9125     0.92875    0.91625    0.93617021 0.91239049]\n",
      "Mean Acc = 0.921212140175219\n"
     ]
    }
   ],
   "source": [
    "text_clf_sgdc = Pipeline([('tfidf', tfidf_vectorizer_swn),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-svm-sgdc', linear_clf_sgdc)])\n",
    "\n",
    "accuracy = cross_val_score(text_clf_sgdc, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDC Classifier without grid search has a higher mean accuracy that Multinomial Naive Bayes Classifier. We will not try to attempt to further enhance it by using parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search for SGDC**\n",
    "parameters_sgdc = {'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "                  'clf-svm-sgdc__alpha': [0.00009, 0.00008],\n",
    "                  'clf-svm-sgdc__penalty': ('l2', 'elasticnet')\n",
    "                 }\n",
    "\n",
    "gs_clf_sgdc = GridSearchCV(text_clf_sgdc, cv=5, param_grid=parameters_sgdc, n_jobs=-1)\n",
    "gs_clf_sgdc = gs_clf_sgdc.fit(train.both, label)\n",
    "\n",
    "print (gs_clf_sgdc.best_score_)\n",
    "print (gs_clf_sgdc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Grid Search is as follows: \n",
    "   \n",
    "       0.942971485743\n",
    "{'clf-svm-sgdv_alpha': 9e-05, 'clf-svm-sgdvpenalty': 'l2', 'tfidf_max_df': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Passive- Aggressive Classifier\n",
    "\n",
    "This particular classifier is from a family of online learning models and is usually used to classify massive streams of data. We are going to attempt to verify the type of accuracy provided by this classifier with this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [0.95       0.94625    0.94       0.95494368 0.94242804]\n",
      "Mean Acc = 0.9467243429286608\n"
     ]
    }
   ],
   "source": [
    "text_clf_pac = Pipeline([('tfidf', tfidf_vectorizer_swn),\n",
    "                         ('chi2', SelectKBest(chi2, k=20000)),\n",
    "                         ('clf-PAS', PassiveAggressiveClassifier(max_iter=50))])\n",
    "\n",
    "accuracy = cross_val_score(text_clf_pac, train.both, label, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Mean Acc = \" + str(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['both'] = test['title'] + test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for test data\n",
    "text_clf_pac.fit(train.both, label)\n",
    "pred = text_clf_pac.predict(test.both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "            \"News_id\": test[\"ID\"],\n",
    "            \"prediction\": pred\n",
    "        },columns = ['News_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store submission dataframe into file\n",
    "submission.to_csv(\"Assignment1_VLN.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier used for final prediction was Passive Agressive Classifier with stopwords removed in the text and using an n-gram of order 2. Stemming was attempted but it did not provide an increase in accuracy. \n",
    "\n",
    "Further investigation could include the following: \n",
    "1. Text Processing\n",
    "    - Lemmatization\n",
    "    - Sentiment Analysis\n",
    "    - Punctuation Analysis (as fake news tends to use a lot more exclamation marks)\n",
    "    \n",
    "2. Classifier\n",
    "    - Boosting Algorithms\n",
    "    - Simple Neural Networks\n",
    "    \n",
    "3. Gathering insights from the classifiers to investigate how exactly it understood the data and performed the classification process. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
